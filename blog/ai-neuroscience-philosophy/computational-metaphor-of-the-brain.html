<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Brain Abstracted: Why AI Might Never Think Like Us</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">
    <link rel="stylesheet" href="../../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
</head>
<body>
    <a href="../../index.html" class="back-link">&larr; Back to Blog</a>

    <article>
        <h1>The Brain Abstracted: Why AI Might Never Think Like Us</h1>

        <p class="blog-meta">Philosophy of Mind &middot; 30 min read</p>
        <hr class="section-break">

        <p>
            As we rush toward Artificial General Intelligence, a philosopher of neuroscience urges us to pause and ask:
            Are we building AI on a fundamental misunderstanding of what minds actually are? This essay is based on a
            fascinating podcast conversation between Tim Scarfe and philosopher <strong>Mazviita Chirimuuta</strong>,
            author of <em>The Brain Abstracted</em>. In their discussion, we'll examine why the drive to create thinking
            machines might be chasing an illusion born from scientific abstraction rather than biological reality.
        </p>

        <p>
            At the heart of Chirimuuta's argument is a critical distinction: comparing the brain to a computer is a
            <strong>useful heuristic</strong>—a scientific shortcut that helps us study complex systems. But it becomes
            a <strong>massive ontological mistake</strong> when we forget the model is a human construction and start
            believing the brain <em>actually is</em> a computer. AI researchers often forget that these computational
            models are designed for our own cognitive limits, not perfect reflections of reality.
        </p>

        <p>
            Chirimuuta, a philosopher with training in neuroscience, argues that while simplification is necessary for
            scientific modeling, our over-reliance on abstraction has led to fundamental misunderstandings about cognition.
            The consequences? Misguided expectations about AGI, technological philosophies that ignore human embodiment,
            and a massive social experiment with our digital lives. Most critically: attempting to achieve AGI by
            recreating an abstracted, disembodied version of the brain fundamentally ignores the biological, metabolic,
            and embodied realities that make human intelligence possible.
        </p>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- VIDEO LINK -->
        <!-- ============================================================ -->

        <div class="note-block" style="text-align: center; padding: 1.5em;">
            <p style="margin: 0 0 1em 0;">
                <strong>Source Material:</strong> This article is based on a conversation between Dr. Tim Scarfe and philosopher Dr. Mazviita Chirimuuta.
            </p>
            <a href="https://www.youtube.com/watch?v=yq318DIwPqw"
               target="_blank"
               rel="noopener noreferrer"
               style="color: #0550ae; text-decoration: none; border-bottom: 1px solid #0550ae;">
                Watch the full podcast on YouTube →
            </a>
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 1: ABSTRACTION AND ITS DANGERS -->
        <!-- ============================================================ -->

        <h2>The Seductive Danger of Abstraction</h2>

        <h3>Abstraction vs. Idealization: When Models Become "More Real" Than Reality</h3>

        <p>
            Science necessarily simplifies the world to make it comprehensible. But Chirimuuta draws a crucial distinction
            between two types of simplification:
        </p>

        <div class="spec-cards">
            <div class="spec-card">
                <div class="term">Abstraction</div>
                <div class="value">Ignoring known details for simplicity</div>
                <div class="note">Example: A physicist calculating Newtonian motion by pretending friction doesn't exist, even though they know it does. In AI: "denoising" messy real-world data to find underlying patterns.</div>
            </div>
            <div class="spec-card">
                <div class="term">Idealization</div>
                <div class="value">Attributing false properties to make calculations tractable</div>
                <div class="note">Example: Genetics models assuming "infinite populations" to make calculations work. In AI: Geometric Deep Learning imbuing systems with perfect geometric priors that don't perfectly map to messy reality.</div>
            </div>
        </div>

        <p>
            The danger emerges when scientists forget these are <strong>representational tools</strong> rather than
            descriptions of reality. A model that ignores the messy complexity of biology can become so elegant, so
            mathematically satisfying, that researchers start to believe the "clean" model is more real than the
            "messy" actual brain. <strong>Jeff Beck, a computational neuroscientist, warns that the denoising process of fitting a machine learning model to neural data can actually strip away the very mechanisms that make the brain work in the real world. The "noise" we filter out might be the "signal" that enables cognition in complex, unpredictable environments.</strong>
        </p>

        <div class="note-block">
            <strong>The Risk:</strong> When abstraction is mistaken for reality, we stop looking for what we've
            abstracted away—even when those "details" might be fundamental to how the system actually works.
        </div>

        <h3>The Mechanistic Metaphor: From Hydraulics to Computers</h3>

        <p>
            Throughout history, our understanding of the mind has shifted through technological metaphors. Each era
            describes the brain using its most sophisticated technology:
        </p>

        <ul>
            <li><strong>17th-18th Century:</strong> Hydraulic machines and clockwork mechanisms</li>
            <li><strong>19th Century:</strong> Telegraphs and electrical switchboards</li>
            <li><strong>20th Century:</strong> Telephone exchanges and analog computers</li>
            <li><strong>Today:</strong> Digital computers and "prediction machines"</li>
        </ul>

        <p>
            These metaphors are not neutral. When we describe the mind as a computer, we make Artificial Intelligence
            seem not just possible but <em>inevitable</em>. After all, if the mind is just a mechanism—just software
            running on biological hardware—then surely we can replicate it in silicon. The metaphor does conceptual
            work, shaping what seems plausible and what research directions receive funding.
        </p>

        <div class="note-block">
            <strong>Critical Question:</strong> What if the brain is not actually a computer in any meaningful sense?
            What if computational models work not because the brain computes, but because computation is a universal
            tool that can approximate many systems?
        </div>

        <h3>The "Lab-to-World" Problem: When Simplified Brains Meet Complex Reality</h3>

        <p>
            Neuroscience data comes from highly controlled laboratory settings. Researchers anesthetize animals,
            immobilize their heads, present simplified stimuli, and record neural activity. This experimental control
            is necessary—it's the only way to isolate variables and establish causal relationships.
        </p>

        <p>
            But here's the trap: <strong>The data is reliable precisely because it represents a simplified brain.</strong>
            In the real world, cognition is defined by complexity and constant interactivity with an unpredictable
            environment. The animal is moving, attending to multiple streams of information, making decisions based on
            internal goals and past experiences.
        </p>

        <p class="key-formula">
            Laboratory Brain → Controlled, Simplified, Measurable<br>
            Real-World Cognition → Interactive, Complex, Contextual
        </p>

        <p>
            When neuroscientists take lab-based findings and try to build a general "Theory of Mind," they often forget
            they've stripped away the very environmental interactivity that makes cognition work. It's like trying to
            understand swimming by studying fish in test tubes.
        </p>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 2: HISTORICAL CASE STUDIES -->
        <!-- ============================================================ -->

        <h2>Lessons from the History of Neuroscience</h2>

        <h3>The Reflex Arc: A "Useful Fiction" That Dominated for Decades</h3>

        <p>
            Chirimuuta examines a revealing historical case: the <strong>reflex arc theory</strong> that dominated
            early 20th-century neuroscience. Researchers like Ivan Pavlov and Charles Sherrington tried to explain
            all behavior as a series of simple reflexes—input-output loops where a stimulus triggers a response.
        </p>

        <div class="memory-hierarchy">
            <div class="memory-tier highlight reg">
                <div class="tier-header">The Simple Reflex Model</div>
                <ul>
                    <li><strong>Stimulus:</strong> Sensory input activates receptor</li>
                    <li><strong>Transmission:</strong> Signal travels through nervous system</li>
                    <li><strong>Response:</strong> Motor output produces behavior</li>
                </ul>
                <p class="note">
                    Sherrington himself admitted the "simple reflex" was likely a useful fiction—an idealization that
                    didn't exist in isolation in living organisms.
                </p>
            </div>
        </div>

        <p>
            Despite Sherrington's own admission that the simple reflex was an idealization, this theory dominated
            behavioral science for decades. Why? Because it made the brain <strong>legible</strong> to scientists.
            It provided a framework for experiments, a vocabulary for discussion, and a sense of progress. The model
            was useful even though it was fundamentally incomplete.
        </p>

        <div class="note-block">
            <strong>The Pattern Repeats:</strong> Today's computational neuroscience may be making the same mistake.
            The "brain as computer" metaphor is useful for experiments and feels scientifically rigorous. But that
            doesn't make it true.
        </div>

        <h3>From Idealized Neurons to Artificial Neural Networks</h3>

        <p>
            The computational metaphor's dominance has deep historical roots. In <strong>1943, Warren McCulloch and
            Walter Pitts</strong> published a groundbreaking paper that took Sherrington's already-idealized concept
            of the neuron and mapped it directly onto <strong>logic gates</strong>. This birthed the artificial neural
            network—a mathematical abstraction of an idealization of a biological neuron.
        </p>

        <p>
            What started as a scientific tool (viewing the brain <em>as if</em> it were a computer to study it) became
            an ontological belief (the brain <em>is</em> a computer). Chirimuuta notes this computational framing gives
            scientists "tunnel vision"—it grants them the license to ignore non-computational biological realities like
            blood flow, immune responses, glial cell functions, and metabolic energy constraints as "irrelevant details."
        </p>

        <p>
            But what if these "irrelevant" biological details are actually central to how cognition works? What if the
            wetware isn't just implementing computation, but doing something fundamentally different that happens to be
            approximated by our computational models?
        </p>

        <h3>The Kaleidoscope Hypothesis: Plato's Ghost in the Machine</h3>

        <p>
            François Chollet, creator of Keras and a prominent AI researcher, has proposed what Chirimuuta calls the
            "Kaleidoscope Hypothesis." Chollet argues that the universe is written in "code"—fundamental rules and
            algorithms—but we perceive it through a kaleidoscope that makes it look messy and complex. AI's task,
            in this view, is to decompose the apparent mess back into the underlying simple rules.
        </p>

        <p>
            Chirimuuta identifies this as fundamentally <strong>Platonic</strong>: the belief that mathematical forms
            are more real than physical embodiment, that "neat" equations are more true than "messy" biology.
        </p>

        <div class="phase-cards">
            <div class="phase-card prefill">
                <h4>Platonic View (Chollet)</h4>
                <p class="what">Reality has simple underlying code; biology is noisy appearance</p>
                <p class="ai"><strong>AI Goal:</strong> Strip away biological "noise" to find pure computational rules</p>
            </div>
            <div class="phase-card decode">
                <h4>Haptic Realism (Chirimuuta)</h4>
                <p class="what">Reality is inherently complex; simplicity is human construction</p>
                <p class="ai"><strong>Human Finitude:</strong> We create simple models because we can't process full complexity</p>
            </div>
        </div>

        <p>
            The problem with "denoising" biological data is that scientists decide what counts as "signal" and what
            counts as "noise." In complex biological systems, the "noise" we filter out might be the exact mechanism
            the system uses to function in the real world. Variability, redundancy, and messiness aren't bugs to be
            removed—they might be features that enable robust, adaptive behavior.
        </p>

        <h3>The God's Eye View vs. Down-to-Earth Realism</h3>

        <p>
            This debate between Platonic and constructivist views traces back to fundamental questions in philosophy.
            Drawing on <strong>Immanuel Kant</strong> and contemporary philosopher <strong>Hasok Chang</strong>
            (author of <em>Realism for Realistic People</em>), Chirimuuta argues for what she calls a
            "down-to-earth" view of scientific knowledge.
        </p>

        <p>
            We don't use mathematical abstraction because the universe is inherently neat and simple—we use it because
            <strong>we are finite beings with cognitive limitations</strong>. Our brains cannot process infinite
            complexity. We construct neat models not because reality is neat, but because we need mental handholds
            to navigate an overwhelmingly complex world. This Kantian insight suggests that our scientific theories
            tell us as much about human cognitive architecture as they do about the universe itself.
        </p>

        <div class="note-block">
            <strong>The Danger of Platonism in AI:</strong> If we believe the universe is fundamentally code waiting
            to be decoded, we'll keep pushing toward AGI as inevitable. But if mathematical models are human constructions
            shaped by our finitude, then AGI based on abstract computation may be fundamentally misconceived.
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 3: HAPTIC REALISM & HUMAN FINITUDE -->
        <!-- ============================================================ -->

        <h2>Haptic Realism: Knowledge Through Touch, Not Sight</h2>

        <h3>Beyond the Spectator Theory of Knowledge</h3>

        <p>
            Traditional philosophy, from ancient Greece through the Enlightenment, has treated knowledge as a form of
            <strong>seeing</strong>—passive observation from a distance. <strong>John Dewey</strong> called this the
            "Spectator Theory" of knowledge: the idea that we discover pre-existing truths by carefully observing nature,
            as if watching a play from the audience.
        </p>

        <p>
            Chirimuuta proposes an alternative: <strong>Haptic Realism</strong>, where knowledge is like
            <strong>touch</strong>. To know something, you must pick it up, meddle with it, change it. Scientific
            knowledge isn't a passive "read-out" of the universe's source code. It's a <em>co-creation</em> that
            results from our physical, embodied interaction with nature.
        </p>

        <p>
            This distinction becomes critical when evaluating AI understanding. Human knowledge is <strong>haptic</strong>—we
            learn by physically touching, manipulating, and interacting with the world. A child learns what "heavy" means
            by trying to lift objects. We understand "hot" through painful experience. Our concepts are grounded in
            sensorimotor interaction.
        </p>

        <p>
            Large Language Models, in stark contrast, are <strong>disembodied "absorbers of facts"</strong> floating in
            the digital cloud. They process vast amounts of text but lack the physical friction required for genuine
            understanding. An LLM can generate perfect descriptions of swimming without ever having experienced water,
            buoyancy, or the effort of staying afloat. Is this understanding, or sophisticated pattern matching?<br/>
            Dewey would argue it's the latter.
        </p>

        <div class="spec-cards">
            <div class="spec-card">
                <div class="term">Visual/Spectator Model</div>
                <div class="value">Knowledge as passive observation</div>
                <div class="note">Truth exists independently; we discover it</div>
            </div>
            <div class="spec-card">
                <div class="term">Haptic/Interactive Model</div>
                <div class="value">Knowledge as active engagement</div>
                <div class="note">Truth is co-created through interaction</div>
            </div>
        </div>

        <h3>Human Finitude: The Limits That Shape Understanding</h3>

        <p>
            <strong>Human finitude</strong> is Chirimuuta's term for our cognitive limits as embodied, mortal beings.
            We can only process so much information, maintain so many variables in working memory, perceive such
            limited ranges of the electromagnetic spectrum. We are finite knowers in an incomprehensibly complex world.
        </p>

        <p>
            Crucially, <strong>we use abstraction not because the universe is simple, but because we cannot process
            its full complexity.</strong> Our models are shaped by what we can measure, what we can compute, what we
            can visualize. These models tell us as much about human cognitive limits as they do about the phenomena
            being modeled.
        </p>

        <div class="note-block">
            <strong>Constructivism:</strong> Scientific theories are not discoveries of pre-existing truth, but
            constructions—products of interactivity between human agents (with specific agendas, tools, and limits)
            and natural phenomena.
        </div>

        <p>
            This doesn't mean science is arbitrary or that truth is relative. It means that scientific models are
            shaped by the finite, embodied agents doing the modeling. Our theories successfully predict phenomena
            not because they've captured ultimate reality, but because they're adequate for human purposes within
            specific domains.
        </p>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 4: CRITIQUING COMPUTATIONALISM -->
        <!-- ============================================================ -->

        <h2>Why the Brain Is Not a Computer</h2>

        <h3>The Ontologization Problem: Don't Mistake the Map for the Territory</h3>

        <p>
            Computational models of the brain have been spectacularly successful. Artificial neural networks can
            recognize faces, translate languages, generate coherent text. Reinforcement learning models successfully
            predict animal behavior in many domains. This success tempts us toward what Chirimuuta calls
            <strong>"ontologization"</strong>—assuming the mathematical structure of the model is the actual
            underlying truth of the biological system.
        </p>

        <p class="key-formula">
            <strong>Successful Computational Modeling ≠ The Brain Is A Computer</strong>
        </p>

        <p>
            Just because a computational model predicts neural activity doesn't mean the brain is performing those
            computations. Computation is a universal tool—given enough parameters and training data, neural networks
            can approximate almost any function. Their success might tell us more about the power of universal
            function approximation than about biological mechanisms.
        </p>

        <h3>The Rock Problem: Computation Alone Lacks Causal Power</h3>

        <p>
            Philosopher Hilary Putnam posed a provocative challenge: if we define computation broadly enough as
            "physical states mapping to logical steps," then theoretically, <strong>a rock</strong> could be mapped
            to perform computation. Any physical system with distinct states can be interpreted as implementing some
            computational process.
        </p>

        <p>
            Following this logic: if computation can be found in a rock, then computation alone cannot explain the
            unique "magic" of the brain. Something more is needed.
        </p>

        <h3>Searle's Chinese Room: The Medium Matters</h3>

        <p>
            <strong>John Searle's</strong> famous <strong>Chinese Room argument</strong> takes this further. Imagine
            someone who doesn't speak Chinese locked in a room with a rule book for manipulating Chinese symbols. They
            receive questions in Chinese, follow the rules to generate responses, and pass them out. From the outside,
            it looks like the room "understands" Chinese. But the person inside is just mechanically following rules—
            symbol manipulation without comprehension.
        </p>

        <p>
            Searle's key point: <strong>computation (symbol manipulation) does not equal understanding (semantics)</strong>.
            More provocatively, he argues that biological brains possess specific <strong>causal powers</strong> that
            silicon chips simply do not. The substrate matters.
        </p>

        <p>
            Chirimuuta agrees with the spirit of Searle's argument: <strong>The medium matters.</strong> If you build an
            AI out of non-living, non-metabolizing materials (silicon), you cannot simply assume it will eventually
            manifest the same cognitive properties—understanding, consciousness, genuine agency—as living biological
            tissue. A simulation of rain doesn't get you wet. A simulation of digestion doesn't extract nutrients. And
            a simulation of a brain might not actually think, no matter how sophisticated the computation.
        </p>

        <div class="note-block">
            <strong>Causal Powers:</strong> Computation itself is causally inert. A mathematical function doesn't
            DO anything—it's an abstract description. Only physical systems have causal powers. So when we say
            "the brain computes," we're describing its behavior from the outside, not explaining the mechanism
            that produces understanding from the inside. The wetware might be doing something fundamentally
            different than computation, even if computation can model its input-output behavior.
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 5: AGENCY & BIOLOGICAL COGNITION -->
        <!-- ============================================================ -->

        <h2>What Makes Biological Minds Different</h2>

        <h3>Agency: The Difference Between Living and Computing</h3>

        <p>
            Biological cognition isn't just information processing—it's shaped by the existential need to survive
            in precarious environments. Living organisms face an ongoing problem: maintain organization in the face
            of entropy, or die. This makes certain information <strong>salient</strong> in a way that has no
            equivalent in artificial systems.
        </p>

        <p>
            A bacterium swimming toward glucose isn't merely responding to a chemical gradient. It's pursuing
            something that matters for its continued existence. The meaning of that gradient is inseparable from
            the organism's status as a living, metabolizing, mortal agent. AI systems don't have this kind of
            existential stake in their computations.
        </p>

        <h3>Dennett's Intentional Stance: A Bridge Too Far?</h3>

        <p>
            Philosopher <strong>Daniel Dennett</strong> argued that if a system acts complex enough, we naturally take
            the <strong>"intentional stance"</strong>—we treat it as if it has beliefs and desires because this helps
            us predict its behavior. By this logic, whether an AI "really" has understanding doesn't matter; if it acts
            intelligently enough, we should treat it as intelligent.
        </p>

        <p>
            Chirimuuta pushes back against applying this stance to AI. Human intentionality, she argues, is fundamentally
            tied to our <strong>physical finitude</strong> and biological imperatives. Our beliefs and desires emerge
            from being mortal creatures with survival needs. Machines don't have biological imperatives; they don't need
            to stay alive. Their "behavior" is optimization toward human-defined loss functions, not self-generated
            meaning emerging from existential necessity.
        </p>

        <p>
            The intentional stance might be useful as a predictive tool ("the chess AI 'wants' to win"), but it
            becomes philosophically misleading when we forget we're using a metaphor and start believing AI systems
            possess genuine agency.
        </p>

        <div class="phase-cards">
            <div class="phase-card prefill">
                <h4>Artificial Systems</h4>
                <p class="what">Process information according to programmed objectives</p>
                <p class="ai"><strong>No Stakes:</strong> System continues functioning regardless of output</p>
                <p class="bottleneck">Meaning is assigned by designers, not intrinsic</p>
            </div>
            <div class="phase-card decode">
                <h4>Biological Systems</h4>
                <p class="what">Process information to maintain existence against entropy</p>
                <p class="ai"><strong>Existential Stakes:</strong> System's survival depends on correct responses</p>
                <p class="bottleneck">Meaning emerges from biological autonomy</p>
            </div>
        </div>

        <h3>Distal vs. Proximal: Memory, Goals, and Temporal Extension</h3>

        <p>
            Biological agents are sensitive to <strong>distal</strong> factors—past memories, future goals,
            anticipated states that don't physically exist in the present. A predator stalking prey isn't merely
            reacting to current sensory input; it's guided by memory of previous hunts, anticipation of the prey's
            likely trajectory, and a goal state (catching the prey) that doesn't yet exist.
        </p>

        <p>
            In contrast, most AI systems are limited to <strong>proximal</strong> inputs—current data, current
            network states. Even when we give them "memory" through mechanisms like attention or recurrent connections,
            this is fundamentally different from biological memory, which is reconstructive, context-dependent, and
            emotionally colored.
        </p>

        <p>
            Biological cognition is <strong>temporally extended</strong> in a way that computation alone cannot
            capture. We are not discrete time-step processors. Our experience unfolds continuously, shaped by
            retention of the just-past and anticipation of the about-to-be.
        </p>

        <h3>The Energy Budget: A Fundamental Divergence</h3>

        <p>
            Perhaps the most striking difference between biological and artificial intelligence: the energy cost.
        </p>

        <div class="spec-cards">
            <div class="spec-card">
                <div class="term">Human Brain</div>
                <div class="value">~20 watts</div>
                <div class="note">About as much as a dim lightbulb</div>
            </div>
            <div class="spec-card">
                <div class="term">GPT-4 Inference Cluster</div>
                <div class="value">Megawatts</div>
                <div class="note">Massive server farms consuming city-scale power</div>
            </div>
            <div class="spec-card">
                <div class="term">Ratio</div>
                <div class="value">100,000:1</div>
                <div class="note">AI uses ~100,000× more power for similar tasks</div>
            </div>
        </div>

        <p>
            This enormous gap suggests biological cognition operates on fundamentally different principles. Evolution
            has been "pruning" neural systems for billions of years, optimizing for energy efficiency in ways we're
            only beginning to understand. The brain isn't just a different architecture running the same computations—
            it may be solving different problems using different principles.
        </p>

        <p>
            AI researchers have discovered the "Lottery Ticket Hypothesis"—that successful networks contain sparse
            subnetworks that could have been trained in isolation. But biological evolution has been running this
            lottery for billions of years, in environments where energy waste means death. The result is not just
            a pruned network but a fundamentally different kind of system.
        </p>

        <div class="note-block">
            <strong>Bio-Economy of Information:</strong> Biological intelligence isn't just optimized computation—
            it's survival-driven information processing under extreme energy constraints, shaped by evolutionary
            time scales that dwarf human engineering.
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 6: TECHNOLOGY & HUMAN NATURE -->
        <!-- ============================================================ -->

        <h2>Technology, Embodiment, and the Cloud Myth</h2>

        <h3>The Immateriality Illusion and the "Infosphere"</h3>

        <p>
            Modern technology marketing presents digital services as existing in "the cloud"—a metaphor that makes
            information seem weightless, infinite, transcending physical limits. Philosopher <strong>Luciano Floridi</strong>
            calls this digital realm the <strong>"infosphere"</strong>—a space where information takes on a life of its
            own, seemingly detached from physical reality. We speak of data "storage" as if bits float in some ethereal
            space, accessible from anywhere, costing nothing.
        </p>

        <p>
            This is a profound denial of reality. Every email, every streamed video, every LLM query runs on
            physical servers in physical data centers, consuming electricity generated from physical fuel sources,
            producing physical heat that requires physical cooling systems. The "cloud" is made of concrete, steel,
            silicon, and rare earth minerals extracted through environmentally destructive mining.
        </p>

        <p>
            But the infosphere isn't just physically expensive—it has real consequences for human lives in ways that
            blur the abstract and the concrete. As journalist <strong>Jon Ronson</strong> documents in
            <em>So You've Been Publicly Shamed</em>, things that exist purely in the abstract digital world
            (angry tweets, viral posts, online mobs) can utterly destroy physical lives—careers ended, families broken,
            mental health shattered. The information world and the physical world are not separate realms.
        </p>

        <p class="key-formula">
            The "Cloud" = Massive Physical Infrastructure<br>
            (We just don't see it or pay the direct costs)
        </p>

        <p>
            Chirimuuta argues this immateriality myth serves a particular agenda: it allows us to imagine
            transcending our physical, embodied nature. If information is immaterial, perhaps consciousness is too.
            If intelligence is just software, perhaps we can upload minds and escape mortality. The technological
            sales pitch becomes metaphysics.
        </p>

        <h3>The Social Experiment: What Are We Doing to Developing Minds?</h3>

        <p>
            Chirimuuta expresses deep concern about the "massive experiment" of social media on the next generation—
            particularly on developing children. Human cognitive development evolved over millions of years in
            environments of face-to-face interaction, "gaze-to-gaze" communication, physical presence.
        </p>

        <p>
            Children learn emotional regulation, social reciprocity, and theory of mind through embodied interaction:
            seeing how their actions affect others' facial expressions, hearing tone of voice, experiencing physical
            comfort and presence. These aren't supplementary features of learning—they may be foundational.
        </p>

        <ul>
            <li><strong>Lost: Physical presence and embodied cues</strong> (facial micro-expressions, body language, touch)</li>
            <li><strong>Lost: Temporal rhythms of face-to-face conversation</strong> (immediate feedback, natural turn-taking)</li>
            <li><strong>Lost: Consequences and accountability</strong> (anonymous online interactions lack real-world stakes)</li>
            <li><strong>Gained: Algorithmic curation of experience</strong> (reality mediated by attention-optimizing systems)</li>
        </ul>

        <p>
            We are conducting this experiment at population scale without control groups, without long-term data,
            driven by business models that optimize for engagement rather than developmental health. Chirimuuta's
            warning: we may be depriving children of the specific forms of social and physical interactivity that
            their brains need to develop healthily.
        </p>

        <div class="note-block">
            <strong>The Embodiment Question:</strong> If biological intelligence is fundamentally embodied—shaped by
            physical sensation, spatial navigation, social touch—what happens when we raise children in increasingly
            disembodied, screen-mediated environments?
        </div>

        <h3>Transcendence and Finitude: The Metaphysical Drive Behind AGI</h3>

        <p>
            Chirimuuta places the pursuit of AGI in a longer philosophical tradition: the drive to transcend human
            embodiment and finitude. From Plato's theory of forms to Christian visions of immaterial souls, from
            Cartesian mind-body dualism to contemporary transhumanism, Western thought has repeatedly sought to
            escape the "limitations" of physical, mortal existence.
        </p>

        <p>
            The dream of Artificial General Intelligence fits this pattern. If we can separate intelligence from
            biological substrate—if we can run minds on silicon—then we've proven consciousness is not bound to
            vulnerable flesh. We've achieved a kind of immortality, at least for information patterns.
        </p>

        <p>
            <strong>Martin Heidegger</strong> warned about our relationship with technology nearly a century ago.
            Chirimuuta extends his critique to our current moment: by treating the world purely as information to be
            processed—scrolling on phones instead of looking out a train window, experiencing life through screens
            instead of through embodied presence—we are changing our own nature.
        </p>

        <p>
            We are creating a <strong>self-fulfilling prophecy</strong>. In our effort to build disembodied, abstract
            computers that think like humans, we are inadvertently training humans to think like disembodied, abstract
            computers. By living increasingly in Floridi's infosphere—texting instead of talking, scrolling instead of
            experiencing, abstracting instead of touching—we become more like the machines we're building. The danger
            isn't that AI will become human. It's that in trying to create human-like AI, we'll make humans more
            machine-like.
        </p>

        <p>
            But Chirimuuta challenges the premise: <strong>our embodiment is not a vestigial limitation to overcome.</strong>
            It's the very foundation of how we know the world. Perception isn't passive reception of data; it's active
            engagement shaped by having a body that moves through space, manipulates objects, needs food and rest, feels
            pleasure and pain.
        </p>

        <div class="phase-cards">
            <div class="phase-card prefill">
                <h4>Transcendence Philosophy</h4>
                <p class="what">Intelligence is substrate-independent information processing</p>
                <p class="ai"><strong>Goal:</strong> Escape physical limits through technology</p>
                <p class="bottleneck">The body is a limitation to transcend</p>
            </div>
            <div class="phase-card decode">
                <h4>Embodied Cognition</h4>
                <p class="what">Intelligence is inseparable from embodied experience</p>
                <p class="ai"><strong>Reality:</strong> Finitude enables particular forms of understanding</p>
                <p class="bottleneck">The body is the condition of possibility for thought</p>
            </div>
        </div>

        <p>
            The irony: our finite, embodied nature isn't what prevents us from knowing the universe completely—
            it's what allows us to know anything at all. A disembodied intelligence with unlimited processing
            power wouldn't think like us at all. It would lack the existential stakes, the survival imperatives,
            the sensorimotor grounding that give meaning to concepts like "danger," "food," "home," "other beings."
        </p>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 7: IMPLICATIONS & CONCLUSIONS -->
        <!-- ============================================================ -->

        <h2>Implications for AI and Philosophy</h2>

        <h3>What This Means for AGI Expectations</h3>

        <p>
            If Chirimuuta is right, then current approaches to AGI are fundamentally misconceived—not because we
            need better algorithms or bigger models, but because they're based on a mistaken understanding of what
            minds are.
        </p>

        <div class="opt-cards">
            <div class="opt-card">
                <strong>What Current AI Can Do:</strong><br>
                <span class="target">Pattern matching, function approximation, statistical prediction within training distributions</span>
            </div>
            <div class="opt-card">
                <strong>What Biological Cognition Does:</strong><br>
                <span class="target">Existentially-grounded meaning-making through embodied interaction with unpredictable environments</span>
            </div>
            <div class="opt-card">
                <strong>The Gap:</strong><br>
                <span class="target">Not a difference in degree (more compute, more data) but a difference in kind</span>
            </div>
        </div>

        <p>
            This doesn't mean AI isn't useful—it IS transformative technology. But it means we should stop expecting
            it to become human-like intelligence. AI might be powerful without being conscious, useful without
            understanding, capable without being alive.
        </p>

        <h3>Taking Embodiment Seriously</h3>

        <p>
            The practical implications of Chirimuuta's philosophy:
        </p>

        <ul>
            <li><strong>For Neuroscience:</strong> Stop assuming computational models that fit data reveal underlying mechanisms. The map is not the territory.</li>
            <li><strong>For AI Research:</strong> Acknowledge that biological intelligence and artificial information processing may be fundamentally different kinds of systems.</li>
            <li><strong>For Technology Design:</strong> Consider embodied human needs rather than treating users as disembodied information processors.</li>
            <li><strong>For Social Policy:</strong> Question the "massive experiment" of raising children in screen-mediated environments.</li>
            <li><strong>For Philosophy:</strong> Reject the Platonic assumption that mathematical elegance equals metaphysical truth.</li>
        </ul>

        <h3>The Philosopher's Warning: Don't Mistake the Map for the Territory</h3>

        <p class="key-formula">
            Computational models are <strong>maps</strong>.<br>
            They are useful <em>because</em> they are simpler than the territory,<br>
            but they are <strong>not</strong> the territory itself.
        </p>

        <p>
            Maps serve human purposes—navigation, planning, communication. They achieve this by <em>omitting</em>
            most details of the actual terrain. A perfectly accurate map at 1:1 scale would be as complex as the
            territory itself, and thus useless. Maps work because they abstract away complexity.
        </p>

        <p>
            Similarly, computational models of the brain abstract away the messy biological details. This makes
            them tractable, testable, and useful for specific purposes. But we shouldn't conclude that the brain
            IS the model, any more than we should conclude that a city IS its subway map.
        </p>

        <div class="note-block">
            <strong>The Central Warning:</strong> If we continue to ignore the biological, embodied, finite nature
            of actual minds, our AI will remain—at best—a sophisticated tool rather than a true replica of human
            intelligence. And at worst, we'll restructure society around a mistaken model of what humans are.
        </div>

        <h3>Final Thoughts: Embracing Finitude and Resisting the Self-Fulfilling Prophecy</h3>

        <p>
            Chirimuuta's ultimate message is both humbling and liberating: <strong>We must take our embodiment and
            finitude seriously—not as limitations to overcome, but as the very foundation of how we know the
            world.</strong>
        </p>

        <p>
            The dream of transcendence—of pure intelligence free from physical constraints—may be fundamentally
            incoherent. What we call "intelligence" might be inseparable from being a flesh-and-blood creature
            with survival needs, navigating a physical environment, shaped by evolutionary history, mortal and
            therefore urgently engaged with the world.
        </p>

        <p>
            But the most urgent warning is this: <strong>We risk creating a devastating self-fulfilling prophecy.</strong>
            The danger isn't that AI will become human. The danger is that in our relentless effort to build machines
            that think like us, we are training ourselves and our children to think like machines. We scroll instead of
            experiencing. We text instead of talking. We abstract instead of touching. We live in Floridi's infosphere
            rather than in embodied presence with one another.
        </p>

        <p>
            If we succeed in this trajectory, we won't have proven that minds are just computation. We'll have
            impoverished human cognition until it fits the computational model we insisted was true all along. The
            computational metaphor will become accurate not because it was always right about the brain, but because
            we'll have reshaped human experience to match our theoretical abstractions.
        </p>

        <p>
            This doesn't mean we should abandon AI research or reject technology. It means we should approach both
            with clear eyes about what they are and aren't. We can build powerful tools without pretending they're
            minds. We can use computation to model brains without believing brains are computers. We can embrace
            technological progress while fiercely protecting the embodied, finite, messy, biological nature of human
            being that makes genuine intelligence—and genuine living—possible.
        </p>

        <hr class="section-break">

        <h3 class="section-takeaway">Key Takeaways</h3>

        <div class="key-points">
            <ul>
                <li><strong>Heuristic vs. Ontology:</strong> The computational metaphor is a useful scientific tool, but a massive mistake when we believe the brain literally is a computer</li>
                <li><strong>Abstraction ≠ Reality:</strong> Scientific models simplify for human purposes; they don't reveal ultimate truth about the system</li>
                <li><strong>AI Relies on Double Idealization:</strong> Neural networks model idealizations (McCulloch & Pitts) of idealizations (Sherrington's reflex arc) of biological neurons</li>
                <li><strong>God's Eye vs. Human Perspective:</strong> We don't use math because the universe is neat (Plato/Chollet); we use it because we're finite beings who need simple models (Kant/Chang)</li>
                <li><strong>Metaphors Matter:</strong> Describing the brain as a computer shapes expectations about AGI in ways that may be misleading</li>
                <li><strong>Lab ≠ World:</strong> Controlled neuroscience experiments study simplified brains; real cognition is irreducibly interactive</li>
                <li><strong>Computation ≠ Causation:</strong> Successful computational modeling doesn't prove the brain computes (Putnam's rock, Searle's Chinese Room)</li>
                <li><strong>The Medium Matters:</strong> Biological tissue may have causal powers that silicon lacks; substrate independence is an unproven assumption</li>
                <li><strong>Function ≠ Implementation:</strong> AI and biological intelligence might achieve similar results through completely different mechanisms</li>
                <li><strong>Intentional Stance Misleads:</strong> Treating AI "as if" it has beliefs (Dennett) becomes problematic when we forget it's a metaphor</li>
                <li><strong>Embodiment Matters:</strong> Biological cognition is fundamentally shaped by having mortal, energy-constrained, moving bodies</li>
                <li><strong>Haptic vs. Spectator Knowledge:</strong> Human understanding requires physical interaction (Dewey); LLMs are disembodied fact absorbers</li>
                <li><strong>Finitude Enables Knowing:</strong> Our limits aren't obstacles to knowledge but conditions of possibility for it</li>
                <li><strong>The Cloud Is Physical:</strong> Floridi's "infosphere" obscures massive material infrastructure and environmental costs</li>
                <li><strong>Digital Harms Are Real:</strong> Abstract online actions (tweets, posts) cause concrete physical consequences (Ronson)</li>
                <li><strong>Children Need Embodied Interaction:</strong> Screen-mediated childhood may deprive developing brains of necessary inputs</li>
                <li><strong>Transcendence Is a Trap:</strong> The drive to escape embodiment through technology continues ancient philosophical errors</li>
                <li><strong>The Self-Fulfilling Prophecy:</strong> In trying to make AI human-like, we risk making humans machine-like (Heidegger's warning realized)</li>
            </ul>
        </div>

        <hr class="section-break">

        <h2>Further Reading</h2>

        <ul>
            <li>Chirimuuta, M. (2023). <em>The Brain Abstracted: Simplification in the History and Philosophy of Neuroscience</em>. MIT Press.</li>
            <li>Chang, H. (2022). <em>Realism for Realistic People: A New Pragmatist Philosophy of Science</em>. Cambridge University Press.</li>
            <li>McCulloch, W. S., & Pitts, W. (1943). "A Logical Calculus of the Ideas Immanent in Nervous Activity." <em>Bulletin of Mathematical Biophysics</em>.</li>
            <li>Dewey, J. (1929). <em>The Quest for Certainty</em>. (On the "Spectator Theory" of knowledge)</li>
            <li>Searle, J. (1980). "Minds, Brains, and Programs." <em>Behavioral and Brain Sciences</em>. (The Chinese Room argument)</li>
            <li>Searle, J. (1992). <em>The Rediscovery of the Mind</em>. MIT Press. (On biological causation and "Strong AI")</li>
            <li>Dennett, D. (1987). <em>The Intentional Stance</em>. MIT Press.</li>
            <li>Putnam, H. (1988). <em>Representation and Reality</em>. (On computational functionalism)</li>
            <li>Floridi, L. (2014). <em>The Fourth Revolution: How the Infosphere is Reshaping Human Reality</em>. Oxford University Press.</li>
            <li>Ronson, J. (2015). <em>So You've Been Publicly Shamed</em>. Riverhead Books.</li>
            <li>Heidegger, M. (1977). <em>The Question Concerning Technology and Other Essays</em>. Harper & Row.</li>
            <li>Thompson, E. (2007). <em>Mind in Life: Biology, Phenomenology, and the Sciences of Mind</em>. (Embodied cognition)</li>
            <li>Varela, F., Thompson, E., & Rosch, E. (1991). <em>The Embodied Mind</em>. (Enactivism and cognitive science)</li>
        </ul>

    </article>

    <footer>
        <p>&copy; 2026 Kieran Schubert &middot; <a href="https://github.com/k-schubert">GitHub</a> &middot; <a href="https://www.linkedin.com/in/kieran-schubert-110772137/">LinkedIn</a></p>
    </footer>

    <script>Prism.highlightAll();</script>
</body>
</html>
