
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPU Fundamentals & LLM Inference</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
</head>
<body>
    <a href="../blogs.html" class="back-link">&larr; Back to Blog</a>

    <article>
        <h1>LLM Inference &amp; GPU Fundamentals</h1>

        <p class="blog-meta">Summary &middot; 30 min read</p>
        <hr class="section-break">

        <p>
            How to reason about LLM inference performance from first principles? In this article, you'll learn how to
            estimate whether a model fits on a given GPU, explain why autoregressive decoding is slow,
            identify bottlenecks (memory bandwidth vs. compute) for a given workload, and propose optimizations that target
            the actual bottleneck.
        </p>
        <p>
            We'll cover GPU architecture, the roofline model, memory estimation,
            arithmetic intensity analysis, and latency estimation, everything you need to develop strong intuition
            about what makes LLM inference fast or slow.
        </p>

        <!-- ============================================================ -->
        <!-- SECTION 1: GPU ARCHITECTURE -->
        <!-- ============================================================ -->

        <h2>GPU Architecture Fundamentals</h2>

        <h3>The NVIDIA H200 NVL 141GB PCIe Gen5 Passive GPU</h3>

        <p>
            The <strong>H200 NVL 141GB PCIe Gen5 Passive GPU</strong> is a solid option for large-scale LLM inference . Understanding its architecture is essential
            for reasoning about performance.
        </p>

        <p>
            Let's start by understanding what each of these terms means:
        </p>

        <ul>
            <li><strong>H200</strong>: NVIDIA's Hopper GPU architecture, improved from the H100 version.</li>
            <li><strong>NVL</strong>: NVL form factor - A PCIe GPU variant designed to be tightly paired with other GPUs using NVLink for high-speed GPU-to-GPU communication (up to 900 GB/s) and shared memory.
            <li><strong>141GB</strong>: The total VRAM capacity of the GPU, using HBM3e memory.</li>
            <li><strong>PCIe Gen5</strong>: The interface standard for connecting the GPU to the CPU, offering relatively slow bandwidth (up to 128 GB/s bidirectional) for data transfer.</li>
            <li><strong>Passive</strong>: The GPU relies on the host system's cooling solution (heatsink and airflow) rather than having an integrated cooling system like SXM modules.</li>
        </ul>

        <p>
            The <strong>NVL form factor</strong> is the standard form factor for inference-focused deployments, as it can be easily added to existing servers without needing 
            specialized cooling or power infrastructure. It is typically used in 2–4-GPU NVLink-paired setups. 
        </p>

        <p>
            The <strong>SXM form factor</strong>, in contrast, is designed for maximum performance and power delivery, but requires custom server designs and liquid cooling, 
            making it more common in training setups. It connects through NVSwitch fabrics, 
            enabling tightly coupled 8-GPU (or larger) configurations with much higher inter-GPU bandwidth.
        </p>

        <!-- ============================================================ -->
        <!-- MEMORY HIERARCHY -->
        <!-- ============================================================ -->

        <h3>GPU Memory Hierarchy</h3>

        <p class="section-intro">
            The H200 NVL PCIe uses the Hopper GH100 architecture. Its memory system is deeply hierarchical:
            tiny ultra-fast on-chip SRAM sits closest to execution units, while massive off-chip HBM3e provides
            capacity but much higher latency. Performance - especially for LLM inference - is governed primarily
            by the bandwidth and latency gaps between these tiers.
        </p>

        <div class="memory-hierarchy">

            <!-- REGISTERS -->
            <div class="memory-tier highlight reg">
                <div class="tier-header">Registers</div>
                <ul>
                    <li><strong>Type:</strong> On-chip SRAM register file</li>
                    <li><strong>Quantity:</strong> 132 register files (one per SM)</li>
                    <li><strong>Capacity:</strong> 256K 32-bit registers per SM (~1 MB/SM; ~132 MB total)</li>
                    <li><strong>Bandwidth:</strong> Essentially unlimited within the SM (single-cycle access)</li>
                    <li><strong>Latency:</strong> ~1 clock cycle</li>
                </ul>
                <p class="note">
                    Registers are the fastest storage and hold thread-private variables. They are allocated per thread.
                </p>
            </div>

            <!-- L1 / SHARED -->
            <div class="memory-tier highlight l1">
                <div class="tier-header">L1 Cache / Shared Memory</div>
                <ul>
                    <li><strong>Type:</strong> On-chip SRAM</li>
                    <li><strong>Quantity:</strong> 132 SM-local units</li>
                    <li><strong>Capacity:</strong> Up to 256 KB per SM (~33 MB total)</li>
                    <li><strong>Bandwidth:</strong> ~20+ TB/s aggregate on-chip bandwidth</li>
                    <li><strong>Latency:</strong> ~25–30 cycles</li>
                </ul>
                <p class="note">
                    This memory can be dynamically partitioned between L1 cache and programmer-managed shared memory.
                    It is critical for tiling, data reuse, and hiding HBM latency.
                </p>
            </div>

            <!-- L2 CACHE -->
            <div class="memory-tier highlight l2">
                <div class="tier-header">L2 Cache</div>
                <ul>
                    <li><strong>Type:</strong> On-chip SRAM</li>
                    <li><strong>Quantity:</strong> Unified global cache shared across all SMs</li>
                    <li><strong>Capacity:</strong> 50 MB</li>
                    <li><strong>Bandwidth:</strong> ~7–8 TB/s internal fabric bandwidth (approx)</li>
                    <li><strong>Latency:</strong> ~150–200 cycles</li>
                </ul>
                <p class="note">
                    L2 acts as a coherence point between SMs and HBM. Large L2 size in Hopper is key for
                    reducing HBM traffic in large-model workloads.
                </p>
            </div>

            <!-- HBM -->
            <div class="memory-tier highlight hbm">
                <div class="tier-header">HBM3e Global Memory</div>
                <ul>
                    <li><strong>Type:</strong> Off-chip stacked DRAM (HBM3e)</li>
                    <li><strong>Quantity:</strong> Multiple HBM stacks on interposer</li>
                    <li><strong>Capacity:</strong> 141 GB total</li>
                    <li><strong>Bandwidth:</strong> 4.8 TB/s peak</li>
                    <li><strong>Latency:</strong> ~350–450 cycles</li>
                </ul>
                <p class="note">
                    This is the primary GPU memory for model weights, activations, and KV cache.
                    Its bandwidth, not compute, is typically the bottleneck during LLM decoding.
                </p>
            </div>

        </div>

        <p>
            During autoregressive token generation, model weights and KV-cache data are first fetched from large, off-chip 
            <strong>HBM</strong> (DRAM) through the GPU’s memory controllers into the shared <strong>L2 cache</strong>, which acts as a 
            high-bandwidth staging buffer for all SMs. From there, active data is pulled into each SM’s on-chip <strong>L1 / shared 
            memory</strong> (SRAM), where it can be reused efficiently by many threads. Finally, the operands are loaded into 
            <strong>registers</strong>, the fastest storage level, and supplied directly to the execution units - including 
            <strong>Tensor Cores</strong> for matrix multiply operations. This hierarchical flow (HBM → L2 → L1/shared → registers → 
            compute units) repeats for every layer and every generated token, and because the largest transfers occur from HBM, 
            the bandwidth of this off-chip memory typically determines overall inference speed.
        </p>

        <div class="memory-pyramid">

            <div class="pyramid-layer reg">
                <div class="layer-content">
                    <strong>Registers</strong><br>
                    On-chip SRAM • ~132 MB total • ~1 cycle latency
                </div>
            </div>

            <div class="pyramid-layer l1">
                <div class="layer-content">
                    <strong>L1 Cache / Shared Memory</strong><br>
                    On-chip SRAM • 256 KB per SM (~33 MB total) • ~20+ TB/s
                </div>
            </div>

            <div class="pyramid-layer l2">
                <div class="layer-content">
                    <strong>L2 Cache</strong><br>
                    On-chip SRAM • 50 MB unified • ~7–8 TB/s internal
                </div>
            </div>

            <div class="pyramid-layer hbm">
                <div class="layer-content">
                    <strong>HBM3e Global Memory</strong><br>
                    Off-chip DRAM • 141 GB • 4.8 TB/s bandwidth
                </div>
            </div>

        </div>

        <h3>SRAM vs DRAM</h3>

        <p>
            Modern GPUs use two fundamentally different types of memory: <strong>SRAM</strong> and <strong>DRAM</strong>. 
            <strong>SRAM (Static RAM)</strong> is built directly on the chip using fast transistor cells, giving it extremely 
            low latency and very high bandwidth, but it is expensive and consumes large area. Because of this, SRAM is used 
            only for small, performance-critical structures such as registers, L1 cache, shared memory, and L2 cache.
        </p>

        <p>
            In contrast, <strong>DRAM (Dynamic RAM)</strong> stores data in tiny capacitor cells that must be periodically 
            refreshed, making it much denser and cheaper but significantly slower. GPUs therefore use DRAM for large-capacity 
            memory like VRAM. Different DRAM technologies exist depending on the workload: 

            <ul>
                <li><strong>HBM</strong> (High Bandwidth Memory) uses stacked chips on an interposer to provide massive bandwidth for data-center GPUs;</li>
                <li><strong>GDDR</strong> (e.g., GDDR6) is optimized for high throughput in consumer graphics cards;</li>
                <li><strong>LPDDR</strong> (e.g., LPDDR5) prioritizes power efficiency for mobile devices.</li>
            </ul>
             
            The large speed and latency gap between fast 
            on-chip SRAM and high-capacity off-chip DRAM is the primary reason memory bandwidth often becomes the bottleneck 
            in large-scale GPU workloads such as LLM inference.
        </p>

        <h3>SRAM vs. HBM: The Crucial Ratio</h3>

        <ul>
            <li><strong>Total SRAM</strong>: ~33 MB (fast, on-chip, ~25–30 cycle latency)</li>
            <li><strong>Total HBM</strong>: 141 GB HBM3e (off-chip, ~350–450 cycle latency)</li>
            <li><strong>Capacity Ratio</strong>: HBM is ~4300x larger but ~13x slower</li>
            <li><strong>Bandwidth Ratio</strong>: ~4.8 TB/s (HBM) vs ~20+ TB/s on-chip aggregate</li>
        </ul>

        <p>
            <div class="note-block">
                This extreme mismatch is the fundamental reason why <strong>memory bandwidth dominates LLM decode performance</strong>.
                During autoregressive generation, every new token requires re-reading model weights from HBM through a
                finite 4.8 TB/s pipe. The larger the model, the more decode becomes memory-bandwidth bound rather than compute-bound.
            </div>
        </p>

        <h3>Memory Hierarchy Latency</h3>

        <div class="memory-stack">
            <div class="memory-tier"><span class="name">Registers</span><span class="latency">~1 cycle</span><span class="size">256K 32-bit regs/SM</span></div>
            <div class="memory-tier"><span class="name">L1 / SRAM</span><span class="latency">~25–30 cycles</span><span class="size">256 KB/SM = ~33 MB total, ~20+ TB/s aggregate</span></div>
            <div class="memory-tier"><span class="name">L2 Cache</span><span class="latency">~150–200 cycles</span><span class="size">50 MB, ~7–8 TB/s internal</span></div>
            <div class="memory-tier"><span class="name">HBM3e</span><span class="latency">~350–450 cycles</span><span class="size">141 GB, 4.8 TB/s</span></div>
        </div>

        <h3>Why This Matters for LLM Inference</h3>

        <p>
            During <strong>autoregressive decode</strong>, each generated token requires reading the full model weights from HBM. We can compute FLOPS
            and bandwidth requirements for a single token generation to see why decode is memory-bound:
        </p>

        <p>
            <div class="key-formula">
                FLOPs ≈ 2 × active parameters (each weight used for one multiply-add).
            </div>

            <ul>
                <li>For gpt-oss-120b, 5.1B active params → <strong>~10 GFLOP</strong> per token.</li>
                <li> For qwen3-8b, 8B active params → <strong>~16 GFLOP</strong> per token.</li>
            </ul>

            <div class="key-formula">  
                Bandwidth = bytes moved from HBM / HBM bandwidth. For FP4, each weight is 0.5 bytes; for FP8, each weight is 1 byte.
            </div>
        </p>

        <ul>
            <li><strong>gpt-oss-120b at MXFP4</strong>: <strong>~2.55 GB</strong> of weights (5.1B × 0.5 bytes)</li>
            <li><strong>At 4.8 TB/s bandwidth</strong>: Takes 2.55 GB / 4.8 TB/s = <strong>0.53 ms</strong> just to read the weights</li>
            <li><strong>Computation</strong>: ~10 GFLOP per token, which at 1'671 TFLOPS takes only <strong>0.006 ms</strong></li>
        </ul>

        <ul>
            <li><strong>qwen3-8b at FP8</strong>: <strong>~8 GB</strong> of weights (8B × 1 byte)</li>
            <li><strong>At 4.8 TB/s bandwidth</strong>: Takes 8 GB / 4.8 TB/s = <strong>1.7 ms</strong> just to read the weights</li>
            <li><strong>Computation</strong>: ~16 GFLOP per token, which at 1'671 TFLOPS takes only <strong>0.01 ms</strong></li>
        </ul>

        <p>
            The weight-loading time is <strong>88x larger</strong> for gpt-oss-120b-mxfp4 and <strong>170x larger</strong> for qwen-3-8b-fp8 than the compute time. This is why decode is
            memory-bandwidth-bound, and why all the major inference optimizations (quantization, KV caching,
            speculative decoding, batching) ultimately aim to reduce the bytes-per-useful-FLOP ratio.
        </p>

        <div class="note-block">
            <strong>Rule of thumb:</strong> If you can do the arithmetic faster than you can load the data, you are
            <strong>memory-bound</strong>. For single-token decode, this is almost always the case.
        </div>

        <p>
            MoE reduces compute cost per token but does not proportionally reduce memory bandwidth requirements, which is why MoE inference often remains memory-bound.
            In each forward pass, we still need to fetch routing data and selected experts' weights from HBM. Memory access can become irregular and reduce cache reuse, 
            further exacerbating bandwidth bottlenecks.
        </p>

        <div class="note-block">
           MoE inference is even <strong>more</strong> memory-bound than dense models.
        </div>

        <h3 class="section-takeaway">Section Takeaways</h3>

        <p class="key-formula">
            The H200 NVL 141GB PCI Gen5 Passive GPU packs <strong>132 SMs</strong>, each with 64 CUDA cores and 4 Tensor cores, connected via <strong>50 MB L2</strong> to <strong>141 GB HBM3e</strong> at 4.8 TB/s. Fast on-chip SRAM (256 KB per SM) sits next to each SM; the size and speed gap between SRAM and HBM is what makes memory bandwidth the bottleneck.
        </p>

        <div class="spec-cards">
            <div class="spec-card"><span class="term">Streaming Multiprocessors</span><div class="value">132</div><div class="note">Each SM is an independent processor</div></div>
            <div class="spec-card"><span class="term">CUDA Cores</span><div class="value">8448 (64/SM)</div><div class="note">Scalar FP ops</div></div>
            <div class="spec-card"><span class="term">Tensor Cores</span><div class="value">528 (4/SM)</div><div class="note">Matrix multiply (FP16/BF16/FP8/INT8)</div></div>
            <div class="spec-card"><span class="term">HBM3e Capacity</span><div class="value">141 GB</div><div class="note">Main VRAM</div></div>
            <div class="spec-card"><span class="term">HBM Bandwidth</span><div class="value">4.8 TB/s</div><div class="note">Data rate from HBM</div></div>
            <div class="spec-card"><span class="term">L2 Cache</span><div class="value">50 MB</div><div class="note">Shared across SMs</div></div>
            <div class="spec-card"><span class="term">L1/Shared per SM</span><div class="value">256 KB</div><div class="note">Fast on-chip</div></div>
            <div class="spec-card"><span class="term">Total SRAM</span><div class="value">~33 MB</div><div class="note">132 &times; 256 KB</div></div>
            <div class="spec-card"><span class="term">FP8 Throughput</span><div class="value">3'341 TFLOPS</div><div class="note">Peak FP8, Tensor Cores</div></div>
            <div class="spec-card"><span class="term">FP16 Throughput</span><div class="value">1'671 TFLOPS</div><div class="note">Peak FP16, Tensor Cores</div></div>

        </div>

        <hr class="section-break">

        <h2>GPU Compute Units</h2>

        <h3>Streaming Multiprocessors (SMs): The GPU’s Processing Engines</h3>

        <p>
            The fundamental compute building block of a GPU is the <strong>Streaming Multiprocessor (SM)</strong>. 
            The H200 NVL contains <strong>132 SMs</strong>, and each SM operates as an independent parallel processor 
            capable of executing thousands of threads concurrently.
        </p>

        <p>
            An SM integrates everything needed to execute GPU programs: execution units, registers, shared memory, 
            scheduling logic, and control hardware. When a kernel runs, thread blocks are assigned to SMs, which then 
            manage execution internally.
        </p>

        <ul>
            <li><strong>Think of an SM as:</strong> a small self-contained parallel CPU</li>
            <li><strong>Each SM contains:</strong> multiple execution pipelines</li>
            <li><strong>All SMs operate simultaneously</strong> to achieve massive throughput</li>
        </ul>

        <h3>Execution Units Inside an SM</h3>

        <p>
            Each Streaming Multiprocessor contains multiple types of execution units designed for different workloads. 
            These specialized units allow the GPU to efficiently process both general-purpose computations and 
            large matrix operations used in AI.
        </p>

        <h4>CUDA Cores: General-Purpose Arithmetic Units</h4>

        <p>
            <strong>CUDA cores</strong> are the primary scalar execution units inside an SM. They perform standard 
            arithmetic operations such as addition, multiplication, logical operations, and control flow. 
            Each CUDA core executes a single operation per clock cycle on one thread.
        </p>

        <ul>
            <li>Optimized for scalar and vector math</li>
            <li>Used heavily in non-AI workloads</li>
            <li>Execute FP32, INT32, and control instructions</li>
        </ul>

        <p>
            During AI inference, CUDA cores still play an important role handling activation functions, 
            indexing logic, and non-matrix computations.
        </p>

        <h4>Tensor Cores: Matrix Multiplication Engines</h4>

        <p>
            <strong>Tensor Cores</strong> are specialized units designed to accelerate matrix multiply-accumulate 
            operations - the core computation in deep learning. Instead of processing one value at a time, 
            they operate on small matrices in parallel, delivering extremely high throughput.
        </p>

        <ul>
            <li>Optimized for AI workloads</li>
            <li>Support FP16, BF16, FP8, INT8, and FP4 formats</li>
            <li>Perform fused matrix multiply-accumulate operations</li>
        </ul>

        <p>
            In LLM inference, Tensor Cores perform nearly all heavy computation, executing the large 
            matrix multiplications that dominate transformer layers.
        </p>

        <h4>Other Specialized Units</h4>

        <p>
            In addition to CUDA and Tensor Cores, each SM includes specialized hardware such as:
        </p>

        <ul>
            <li><strong>Load/Store units</strong> for moving data between memory levels</li>
            <li><strong>Special Function Units (SFUs)</strong> for transcendental math (exp, sin, etc.)</li>
            <li><strong>Scheduling hardware</strong> that dispatches warps to execution pipelines</li>
        </ul>

        <h3>Data Flow Inside an SM During LLM Inference</h3>

        <p>
            During transformer inference, data moves through several stages inside each SM. Model weights 
            and activations are loaded from global memory into on-chip caches, then staged in registers. 
            These operands are finally fed into Tensor Cores, where matrix multiplications are performed. 
            The results are written back to registers and eventually stored in global memory.
        </p>

        <p>
            Because Tensor Cores can process data far faster than it can be delivered from HBM, the speed 
            of memory movement - not computation - typically limits overall inference performance.
        </p>

        <h3>Why GPUs Use Different Types of Compute Units</h3>

        <p>
            GPUs include both scalar CUDA cores and matrix-focused Tensor Cores because different workloads 
            require different computational patterns. Traditional graphics and scientific computing rely on 
            scalar arithmetic, while modern AI workloads are dominated by large matrix multiplications.
        </p>

        <p>
            Tensor Cores provide orders-of-magnitude higher throughput for these operations by processing 
            entire matrix tiles in parallel, whereas CUDA cores handle the surrounding non-matrix logic.
        </p>

        <hr class="section-break">
        
        <h2>GPU Execution Model</h2><br>

        <h3>Kernels: Programs That Run on the GPU</h3>

        <p>
            A <strong>kernel</strong> is a function executed on the GPU. When a kernel is launched, it runs 
            many times in parallel across thousands of threads. Each thread executes the same instructions 
            but operates on different data, enabling massive data parallelism.
        </p>

        <p>
            In AI inference, kernels implement operations such as matrix multiplication, attention, 
            normalization, and activation functions.
        </p>

        <h3>Threads: The Smallest Execution Units</h3>

        <p>
            A <strong>thread</strong> is the smallest unit of execution in a GPU program. Each thread 
            executes the same kernel code but works on a different portion of data. Threads maintain 
            their own registers and private variables.
        </p>

        <p>
            Modern GPUs can run millions of threads concurrently to fully utilize available compute units.
        </p>

        <h3>Thread Blocks: Groups of Cooperative Threads</h3>

        <p>
            Threads are organized into <strong>thread blocks</strong>, which execute on a single SM. 
            Threads within a block can synchronize and share data using fast on-chip shared memory.
        </p>

        <p>
            Blocks are independent of one another, allowing them to be scheduled flexibly across SMs.
        </p>

        <h3>Grids: The Full Kernel Execution</h3>

        <p>
            A <strong>grid</strong> is the complete collection of thread blocks launched by a kernel. 
            The GPU distributes these blocks across all available SMs, enabling large-scale parallel execution.
        </p>

        <p>
            This hierarchical structure (grid → blocks → threads) allows GPU programs to scale efficiently 
            across many processing units.
        </p>

        <h3>Waves: How Work Flows Across SMs</h3>

        <p>
            Because a GPU has a limited number of SMs, not all thread blocks can run simultaneously. 
            Instead, blocks are executed in groups called <strong>waves</strong>. Once one wave finishes, 
            the next wave is scheduled onto the freed SMs.
        </p>

        <p>
            Large kernels may require many waves to complete, and overall runtime depends on how efficiently 
            these waves keep the GPU fully occupied.
        </p>

        <h3>Execution Model: Warps and Thread Blocks</h3>

        <ul>
            <li><strong>Kernel launch</strong> &rarr; a grid of thread blocks is scheduled onto SMs.</li>
            <li><strong>Each block</strong> has up to 1024 threads, split into warps.</li>
            <li><strong>Each warp</strong> = 32 threads executing in lockstep (SIMT); they all run the same instruction at once.</li>
        </ul>

        <p>
            The <strong>warp</strong> is the fundamental unit of execution. All 32 threads in a warp execute the same
            instruction at the same time. If threads diverge (different <code>if</code> branches), both paths must be
            executed serially , this is called <strong>warp divergence</strong> and it wastes cycles.
        </p>

        <h3>SIMT vs SIMD: How GPUs Execute Parallel Instructions</h3>

        <p>
            GPUs use a programming model called <strong>SIMT (Single Instruction, Multiple Threads)</strong>, 
            which is often confused with <strong>SIMD (Single Instruction, Multiple Data)</strong>. While both 
            execute the same instruction across many data elements, they differ in how parallelism is exposed.
        </p>

        <p>
            In <strong>SIMD</strong>, a single instruction operates on a fixed vector of data within one processor, 
            such as CPU vector units (AVX). All elements must follow the same control flow.
        </p>

        <p>
            In contrast, <strong>SIMT</strong> executes many independent threads that each maintain their own 
            registers and program counters. These threads are grouped into warps, which execute instructions 
            together in lockstep. If threads in a warp take different execution paths, the GPU must serialize 
            those paths, leading to reduced efficiency known as <strong>warp divergence</strong>.
        </p>

        <p>
            This SIMT model provides far greater flexibility than traditional SIMD, allowing GPUs to efficiently 
            handle complex, irregular workloads such as large-scale AI inference.
        </p>


        <h3>Parallelism and Occupancy: Keeping the GPU Busy</h3>

        <p>
            Each SM can execute thousands of threads simultaneously by rapidly switching between warps. 
            This ability to keep execution units busy while waiting for memory operations is known as 
            <strong>occupancy</strong>. High occupancy allows GPUs to hide memory latency and maintain 
            high throughput.
        </p>

        <p>
            Achieving high occupancy is critical for maintaining peak performance in memory-bound workloads 
            such as large language model inference.
        </p>

        <div class="note-block">
            <strong>Occupancy</strong> refers to how many threads are active on an SM at a given time. 
            High occupancy allows the GPU to hide memory latency by switching to other threads while 
            waiting for data to arrive.
        </div>

        <hr class="section-break">

        

    </article>

    <footer>
        <p>&copy; 2026 Kieran Schubert &middot; <a href="https://github.com/k-schubert">GitHub</a> &middot; <a href="https://www.linkedin.com/in/kieran-schubert-110772137/">LinkedIn</a></p>
    </footer>

    <script>Prism.highlightAll();</script>
</body>
</html>
